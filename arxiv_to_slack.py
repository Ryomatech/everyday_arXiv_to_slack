#!/usr/bin/env python3
# arxiv_to_slack.py
import os
import feedparser
import datetime
from datetime import timedelta
import json
import requests
import pathlib
import urllib.parse
import time
from ml_keywords import ML_KEYWORDS, MATERIAL_KEYWORDS, LLM_KEYWORDS

# ArXiv APIã®URL
ARXIV_API_BASE = "http://export.arxiv.org/api/query?"

# æ¤œç´¢å¯¾è±¡ã®ã‚«ãƒ†ã‚´ãƒªã¨å¯¾å¿œã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
SEARCH_CATEGORIES = {
    "cond-mat.mtrl-sci": ML_KEYWORDS,  # ç‰©æ€§ãƒ»ææ–™ç§‘å­¦ã§ã¯æ©Ÿæ¢°å­¦ç¿’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿
    "cs.AI": MATERIAL_KEYWORDS,  # CS.AIã§ã¯ææ–™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿
    "cs.LG": MATERIAL_KEYWORDS,  # CS.LGã§ã¯ææ–™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿
    "cs.CL": MATERIAL_KEYWORDS,  # CS.CLã§ã¯ææ–™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿
    # LLMé–¢é€£ã®è«–æ–‡ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®è¿½åŠ ã‚«ãƒ†ã‚´ãƒª
    "cs.AI-LLM": LLM_KEYWORDS,  # CS.AIã‹ã‚‰LLMé–¢é€£è«–æ–‡
    "cs.LG-LLM": LLM_KEYWORDS,  # CS.LGã‹ã‚‰LLMé–¢é€£è«–æ–‡
    "cs.CL-LLM": LLM_KEYWORDS,  # CS.CLã‹ã‚‰LLMé–¢é€£è«–æ–‡
}

WEBHOOK = os.environ.get("SLACK_WEBHOOK_URL")


def fetch_new_entries_for_category(category: str, keywords: list):
    """æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã‹ã‚‰æ–°ã—ã„ã‚¨ãƒ³ãƒˆãƒªã‚’å–å¾—"""
    try:
        # ç¾åœ¨ã®UTCæ™‚åˆ»ã‚’å–å¾—
        now = datetime.datetime.utcnow()
        # 24æ™‚é–“å‰ã®UTCæ™‚åˆ»ã‚’å–å¾—
        time_threshold = now - timedelta(hours=24)

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§abstractãƒ•ã‚£ãƒ«ã‚¿ã‚’æ§‹ç¯‰
        keyword_terms = [f'abs:"{keyword}"' for keyword in keywords]
        keyword_filter = " OR ".join(keyword_terms)

        # LLMå°‚ç”¨ã‚«ãƒ†ã‚´ãƒªã®å ´åˆã€å®Ÿéš›ã®arXivã‚«ãƒ†ã‚´ãƒªåã‚’ä½¿ç”¨
        actual_category = category
        if category.endswith("-LLM"):
            actual_category = category.replace("-LLM", "")

        # ã‚«ãƒ†ã‚´ãƒªã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’çµ„ã¿åˆã‚ã›ãŸæ¤œç´¢ã‚¯ã‚¨ãƒª
        search_query = f"cat:{actual_category} AND ({keyword_filter})"

        # APIãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
        params = {
            "search_query": search_query,
            "start": 0,
            "max_results": 100,
            "sortBy": "submittedDate",
            "sortOrder": "descending",
        }

        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’URLã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ã‚¯ã‚¨ãƒªæ–‡å­—åˆ—ã‚’ä½œæˆ
        query_string = urllib.parse.urlencode(params, safe=":")
        url = ARXIV_API_BASE + query_string

        print(f"Fetching from {category} (actual: {actual_category}): {url}")

        # ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’ãƒ‘ãƒ¼ã‚¹
        feed = feedparser.parse(url)

        new_entries = []
        for entry in feed.entries:
            # 'published' ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ—¥ä»˜ã‚’è§£æ
            published_str = entry.published
            published = datetime.datetime.strptime(published_str, "%Y-%m-%dT%H:%M:%SZ")

            # æŒ‡å®šæ™‚é–“å†…ã«å…¬é–‹ã•ã‚ŒãŸã‚‚ã®ã‹ã‚’ç¢ºèª
            if published >= time_threshold:
                new_entries.append(entry)

        # APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ã¦å°‘ã—å¾…æ©Ÿ
        time.sleep(1)

        # å¤ã„é †ã«ä¸¦ã¹æ›¿ãˆã¦è¿”ã™
        return list(reversed(new_entries))

    except Exception as e:
        print(f"Error fetching entries for {category}: {e}")
        return []


def get_category_emoji(category: str) -> str:
    """ã‚«ãƒ†ã‚´ãƒªã«å¯¾å¿œã™ã‚‹çµµæ–‡å­—ã‚’è¿”ã™"""
    emoji_map = {
        "cond-mat.mtrl-sci": "ğŸ”¬",
        "cs.AI": "ğŸ¤–",
        "cs.LG": "ğŸ“Š",
        "cs.CL": "ğŸ’¬",
        # LLMå°‚ç”¨ã‚«ãƒ†ã‚´ãƒªç”¨ã®çµµæ–‡å­—
        "cs.AI-LLM": "ğŸ§ ",
        "cs.LG-LLM": "ğŸ¤–ğŸ’¬",
        "cs.CL-LLM": "ğŸ“",
    }
    return emoji_map.get(category, "ğŸ“„")


def build_message_for_category(category: str, entries: list) -> str:
    """æŒ‡å®šã‚«ãƒ†ã‚´ãƒªã®Slackç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ–‡å­—åˆ—çµ„ã¿ç«‹ã¦"""
    if not entries:
        return ""

    emoji = get_category_emoji(category)
    lines = [f"{emoji} *{category} æ–°ç€è«–æ–‡ ({len(entries)}ä»¶)*"]

    for e in entries:
        title = e.title.replace("\n", " ")
        # abstractãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚’ä½¿ç”¨
        abs_url = e.link  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§abstractãƒšãƒ¼ã‚¸ã®URL

        # ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’å–å¾—
        categories = (
            ", ".join(tag["term"] for tag in e.tags) if hasattr(e, "tags") else category
        )

        lines.append(f"â€¢ *{title}*")
        lines.append(f"  ğŸ“„ <{abs_url}|Abstract> | ğŸ·ï¸ {categories}")
        lines.append("")  # ç©ºè¡Œã§åŒºåˆ‡ã‚Š

    return "\n".join(lines)


def post_to_slack(text: str):
    """Webhook ã§ãƒã‚¹ãƒˆ"""
    if not WEBHOOK:
        print("SLACK_WEBHOOK_URL environment variable is not set")
        return

    try:
        resp = requests.post(WEBHOOK, json={"text": text})
        resp.raise_for_status()
        print("Successfully posted to Slack")
    except Exception as e:
        print(f"Error posting to Slack: {e}")


if __name__ == "__main__":
    all_messages = []

    # ç¾åœ¨ã®æ—¥ä»˜ã‚’å–å¾—ï¼ˆJSTï¼‰
    jst = datetime.timezone(datetime.timedelta(hours=9))
    current_date = datetime.datetime.now(jst).strftime("%m/%d")

    # å„ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰è«–æ–‡ã‚’å–å¾—
    for category, keywords in SEARCH_CATEGORIES.items():
        print(f"Processing category: {category}")
        entries = fetch_new_entries_for_category(category, keywords)
        print(f"Found {len(entries)} entries for {category}")

        if entries:
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä½œæˆ
            message = build_message_for_category(category, entries)
            if message:
                all_messages.append(message)

    # ã™ã¹ã¦ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’çµåˆã—ã¦é€ä¿¡
    if all_messages:
        final_message = f"ğŸ“… *arXivæ›´æ–°æƒ…å ± ({current_date})*\n\n" + "\n\n".join(
            all_messages
        )
        print(f"Message: {final_message}")
        post_to_slack(final_message)
    else:
        no_papers_message = f"ğŸ“… *arXivæ›´æ–°æƒ…å ± ({current_date})*\nğŸ“š éå»24æ™‚é–“ä»¥å†…ã«æ–°ç€è«–æ–‡ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"
        print(f"Message: {no_papers_message}")
        post_to_slack(no_papers_message)
